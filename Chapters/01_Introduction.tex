\chapter{Introduction}

\section{Background \& Motivation}

The rapid expansion of the Internet of Things (IoT) ecosystem has led to widespread deployment of embedded devices equipped with sensors and low-power microcontrollers. These devices are expected to operate continuously in energy- and memory-constrained environments, often disconnected from cloud services. Offloading raw sensor data to centralized servers for processing introduces latency, energy costs, and privacy risks. This has sparked significant interest in \textbf{Tiny Machine Learning (TinyML)}—a field that focuses on running neural networks entirely on-device.

TinyML enables applications such as on-device speech recognition, gesture detection, and image classification, especially in always-on sensors, wearables, and smart home devices. To make this feasible, researchers have developed compact and efficient deep learning models tailored to edge hardware. \textbf{MobileNets}, for example, leverage depthwise separable convolutions to build lightweight convolutional neural networks (CNNs) that perform well on mobile vision tasks while maintaining low computational cost~\cite{ConvNetworksMobileNets}. \textbf{EfficientNet} extends this idea by using compound scaling to balance depth, width, and resolution for optimal accuracy under resource constraints~\cite{tan2019efficientnet}.

Other techniques such as \textbf{network pruning} and \textbf{quantization} further compress models by eliminating redundant parameters and reducing numeric precision, respectively. Together, these strategies have significantly advanced \textbf{resource-efficient deep learning}, enabling inference on smartphones and edge devices with modest resources~\cite{alajlan2022tinyml}.

Despite these advances, TinyML deployments face \textbf{tight hardware limits}. For example, the Arduino Nano 33 BLE Sense has only 256\,KB of RAM and 1\,MB of flash memory. Under such constraints, even a quantized MobileNet-V2 may exceed available memory. These conditions demand highly compact architectures and efficient inference libraries optimized for microcontrollers.

While hand-designed architectures have made remarkable progress, manually crafting efficient models is time-consuming and often suboptimal. \textbf{Neural Architecture Search (NAS)} was proposed as a solution to this challenge: it automates the design of high-performance neural networks by systematically exploring large architecture spaces~\cite{liu2022survey}. NAS has produced architectures that match or surpass human-designed models on image classification benchmarks like CIFAR-10 and CIFAR-100.

However, NAS remains \textbf{computationally expensive}. For example, Liu et al. used a reinforcement learning–based NAS framework to design a model for CIFAR-10, requiring 28 days of computation on 800 GPUs~\cite{liu2022survey}. Such computational demands make NAS inaccessible to most researchers and wholly unsuitable for deployment on-device. Moreover, most NAS frameworks are not designed to be hardware-aware; they do not account for the strict memory, latency, and energy constraints of microcontroller-class devices.

This thesis is motivated by the need to make NAS both \textbf{efficient} and \textbf{hardware-aware}. We aim to explore novel strategies—such as lightweight evaluation proxies and search-space constraints—that can reduce redundant computation, guide the search more intelligently, and produce architectures that are deployable on real-world embedded platforms. By developing a more efficient NAS pipeline, this work contributes to making automated model design feasible for TinyML applications.

\section{Problem Statement}

The growth of TinyML has unlocked the possibility of deploying deep learning models directly on resource-constrained embedded devices. However, designing neural network architectures that are both accurate and lightweight enough to run on microcontrollers remains a significant challenge.

Transmitting sensor data to the cloud is costly in terms of latency, energy, and privacy. Running inference locally avoids these costs but requires models that are extremely compact and efficient. Techniques such as quantization, pruning, and architectural innovations like depthwise convolutions have made it possible to shrink models. Yet, these approaches often rely on \textbf{manual design}, which is time-intensive and may not yield optimal results for all tasks or devices.

\textbf{Neural Architecture Search (NAS)} offers a solution by automating the model design process. However, existing NAS approaches are not well-suited for embedded environments:
\begin{itemize}
    \item They typically require \textbf{significant computational resources}—many GPU-days or weeks of training.
    \item They often \textbf{ignore hardware constraints}, failing to enforce limits on memory usage, parameter count, or inference latency during the search.
\end{itemize}

These shortcomings make NAS impractical for TinyML scenarios and limit its adoption in low-power, memory-constrained platforms like the Arduino Nano 33 BLE.

Thus, the \textbf{core problem} addressed in this thesis is twofold:

\begin{enumerate}
    \item \textbf{How can we drastically accelerate NAS}, making it computationally feasible for small-scale research and practical deployment?
    \item \textbf{How can we incorporate hardware-awareness into the NAS process}, ensuring that discovered architectures meet strict constraints in terms of memory, latency, and computational budget?
\end{enumerate}

Solving these challenges will help bridge the gap between NAS research and real-world deployment on microcontrollers, bringing automated deep learning closer to the edge.








\section{Research Objectives}

The primary objective of this research is to develop a resource-efficient Neural Architecture Search (NAS) framework capable of rapidly discovering and deploying optimized neural network models on constrained embedded platforms. Specifically, the research aims to accelerate a NAS algorithm, based on a Genetic Algorithm (GA), tailored for the CIFAR-100 image classification dataset, while ensuring that the resulting models are efficient enough to run on microcontroller hardware.

To achieve this overarching goal, the following specific research objectives are pursued:
 
\begin{enumerate}
    \item \textbf{Design a custom NAS algorithm using a Genetic Algorithm (GA):} Develop a NAS approach that leverages evolutionary strategies to automatically explore convolutional neural network architectures for the CIFAR-100 dataset (which consists of 60,000 32$\times$32 images across 100 classes~\cite{cifar}). 
    
    The choice of a GA is motivated by its ability to perform global search and handle multi-objective optimization, and by prior successes of evolutionary methods in architecture search. This objective includes defining an appropriate search space and genetic encoding for candidate network architectures.

    \item \textbf{Accelerate the NAS search process:} Improve the speed and computational efficiency of the NAS algorithm so that high-performing network architectures can be identified with significantly reduced search time. Conventional NAS techniques are notoriously time-consuming and computationally expensive---for example, early NAS experiments required on the order of tens of thousands of GPU-hours~\cite{pham2018efficient}. In this work, the NAS algorithm is optimized to converge more quickly to promising architectures. The goal is to reduce the overall time and resources needed for the search, making NAS feasible without access to vast compute resources.



    \item \textbf{Deploy and evaluate the optimized model on hardware:} Take the best evolved neural network architectures and deploy them on the Arduino Nano 33 BLE platform to validate their real-world performance. This involves converting or quantizing the model (e.g., via TensorFlow Lite for Microcontrollers) and measuring on-device metrics such as inference latency, memory usage (RAM/Flash consumption), and classification accuracy. By profiling the deployed model, the research can confirm that the networks indeed operate within the device’s memory and processing limits. Analyzing these performance metrics will also help in identifying any bottlenecks (e.g., additional RAM and Flash consumption other than our model) and in demonstrating that the project’s improvements meet the targeted objectives. In summary, this objective ensures that the NAS-generated model is not only theoretically efficient but also practically viable on a real embedded device, thereby closing the loop from algorithm design to embedded deployment.
\end{enumerate}


\section{Scope}

The scope of this thesis is deliberately limited to a well-defined application area and methodological framework, as outlined below:

\subsection{Target Problem and Dataset}

This work is centered on an image classification task using the CIFAR-100 dataset. All experiments and evaluations use CIFAR-100 as the benchmark problem, and the NAS algorithm is specifically tailored to this dataset. Other datasets or machine learning tasks (e.g., CIFAR-10, ImageNet, object detection, or speech recognition) are outside the scope of this research. Focusing on CIFAR-100—a relatively complex vision dataset with 100 classes—provides a challenging test case for the NAS algorithm, while remaining tractable for experimentation within the resource and time constraints of a master’s thesis.

\subsection{Neural Architecture Search Methodology}

The research employs a custom Genetic Algorithm (GA) for neural architecture search. This evolutionary approach to NAS is the sole search strategy implemented and studied in depth. Alternative NAS techniques—such as reinforcement learning-based controllers or differentiable NAS methods—are not implemented in this project. These methods may be discussed in the literature review for context, but no comparative analysis is conducted. The decision to use a GA defines the methodological scope: all improvements and observations are made in the context of evolutionary search. The thesis does not attempt to develop new reinforcement learning algorithms or gradient-based NAS; instead, it strictly investigates how a GA-driven NAS can be accelerated and made hardware-aware.

\subsection{Embedded Deployment Focus}

The resulting neural network models are designed specifically for deployment on a microcontroller-based platform: the Arduino Nano 33 BLE. All optimization efforts (e.g., model compression, quantization) and evaluations are geared toward this embedded device environment. The Arduino Nano 33 BLE board, featuring an Arm Cortex-M4F MCU running at 64~MHz with 1~MB flash and 256~KB RAM~\cite{ArduinoNano}, serves as the representative target for “resource-constrained embedded devices.” The scope includes using standard embedded ML toolchains, such as TensorFlow Lite for Microcontrollers, to run inference on the device. Τhe deployment scope to a single device, the research can deeply analyze the model’s performance under consistent and well-understood resource constraints.

\clearpage

\subsection{Performance Metrics}

This research defines a clear set of metrics to evaluate both the quality and practical viability of the neural network models generated through the NAS process. These metrics are chosen to align closely with the constraints and goals of deploying deep learning models on ultra-low-power embedded systems. The metrics are considered at two key stages of the workflow:
\begin{itemize}
    \item During the NAS process, to guide architecture selection and pruning via predictive models and constraint checks.
\end{itemize}
\begin{itemize}
    \item After deployment on hardware, to validate actual on-device behavior and efficiency.
\end{itemize}
The primary performance metrics are the following:

\textbf{Inference latency}

This refers to the time it takes for the deployed model to process an input and produce a prediction on the Arduino Nano 33 BLE. Latency is critical in real-time embedded applications—such as gesture recognition, audio processing, or sensor data classification—where delayed responses can compromise functionality or user experience. Measuring latency ensures that the model is fast enough for time-sensitive applications.

\textbf{Memory usage}

Memory efficiency is assessed in two dimensions:
\begin{itemize}
    \item \textbf{\textit{Static memory usage (Flash)}:} This includes the size of the compiled model binary (weights, parameters, and code) stored in the microcontroller's non-volatile flash memory. The total must remain under the 1MB limit of the Arduino Nano 33 BLE.

    \item \textbf{\textit{Runtime memory usage (RAM)}:} This includes memory used during inference (activations, buffers, stack, and heap). Since the target device only has 256KB of RAM, models must be sufficiently compact to run within this tight constraint. Excessive RAM usage can lead to runtime errors or unpredictable behavior.
\end{itemize}

\textbf{Classification accuracy}

Although the focus is on efficiency, model accuracy on the CIFAR-100 dataset remains a key metric. The objective is to maintain as high accuracy as possible while minimizing model complexity. This trade-off between performance and resource usage is at the core of this thesis, and all evaluated architectures are measured for their top-1 classification accuracy on a held-out test set.

\subsection{\textbf{Limitations and Exclusions}}

Certain performance aspects are acknowledged but fall outside the scope of this study due to resource or technical limitations:

\paragraph{\textbf{Energy Consumption}:}

While energy efficiency is a critical concern for embedded systems, direct power measurements (e.g., in milliwatts or joules) are not performed. This is due to the lack of precise measurement tools and time constraints within the thesis. However, energy usage is indirectly optimized, as reducing model size, computation, and memory access typically leads to lower power draw. Thus, the design choices made in this work still promote energy efficiency, even if not quantitatively evaluated.

\TODO{NOT YET MEASURED BUT IT IS PLANNED}


\paragraph{\textbf{On-Device Training}:}
The thesis does not address training on the microcontroller. All model training and architecture search are conducted using conventional computing systems (e.g., desktop or GPU-accelerated environments). The embedded device is used exclusively for inference, which reflects current practical use cases, as microcontrollers typically lack the computational capacity for full model training.

\subsection{\textbf{Summary}}

By focusing on these well-defined and measurable performance metrics, the thesis ensures that the proposed NAS framework and the resulting models are not only theoretically sound but also practically deployable. The approach emphasizes real-world constraints, which are often overlooked in academic work, thereby bridging the gap between deep learning research and embedded systems deployment.


