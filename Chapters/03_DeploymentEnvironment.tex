\chapter{Deployment Environment}
\label{chap:DeploymentEnvironment}
\section{Description}

In this thesis, our objective is to deploy the proposed model on a highly constrained edge device\textemdash the \textbf{Arduino Nano 33 BLE Sense}. This microcontroller board features an Arm Cortex-M4 CPU with limited RAM and Flash memory, making it a challenging yet meaningful platform for evaluating the efficiency of neural network architectures optimized via Neural Architecture Search (NAS). The primary goal is to demonstrate that a well-structured lightweight model can achieve high accuracy while remaining deployable on low-power hardware.



\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{Pictures/Arduino image.png}
    \caption{Example of an Arduino Nano family microcontroller board. Modern TinyML deployments target devices like the Arduino Nano 33 BLE Sense, which feature low-power CPUs but only on the order of a few hundred kilobytes of SRAM and a few megabytes of flash memory. }
    \label{fig:arduino_nano}
\end{figure}






\section{Arduino Nano 33 BLE Sense Specifications}

The Arduino Nano 33 BLE Sense is a compact, low-power board designed for embedded AI applications. The following are the key technical specifications:

\begin{itemize}
\item \textbf{Processor:} 64 MHz Arm Cortex-M4 CPU with FPU
\item \textbf{RAM:} 256 KB
\item \textbf{Flash Memory:} 1 MB
\item \textbf{Connectivity:} Bluetooth 5.0 Low Energy
\item \textbf{Sensors:} 9-axis IMU, microphone, temperature, humidity, pressure, color, gesture
\item \textbf{Power Consumption:} Extremely low, suitable for battery-powered deployments
\end{itemize}

Due to these hardware constraints, deploying deep learning models on this device requires careful optimization of memory usage and computational complexity. 

The model itself—meaning the weights and the compiled operations—is stored in the non-volatile \textbf{Flash memory}. Therefore, the total model size must not exceed the available 1 MB of Flash. In contrast, \textbf{RAM} is volatile and used at runtime to store intermediate tensors, activations, input/output buffers, and temporary computation data. During inference, a structure known as the \texttt{tensor arena} is allocated in RAM to support these operations. This arena must be statically pre-allocated and sized conservatively to avoid memory allocation failures, while still allowing the model to run effectively.

Understanding the difference between these two memory types is crucial:
\begin{itemize}
\item \textbf{Flash memory} retains data after power-off and is used to store the model and program code.
\item \textbf{RAM} is cleared on reboot and is used dynamically during inference for data processing.
\end{itemize}

Given the limited 256 KB of RAM, a large portion must be reserved for the tensor arena. This requires balancing model complexity with available runtime memory. Optimizing both Flash and RAM usage is essential to ensure the model fits within the constraints of the device and executes reliably.


\section{Deployment Process}
\label{sec:DeploymentProcess}
To deploy the quantized TensorFlow Lite (TFLite) model on the Arduino Nano 33 BLE Sense, a C++ sketch is written in the \texttt{Arduino IDE}. This sketch integrates TensorFlow Lite for Microcontrollers (TFLM) and includes logic to load the model, allocate memory, run inference, and output results. 

The critical components of this deployment are:

\begin{itemize}
    \item \textbf{Tensor Arena:} 
    A statically allocated memory buffer used by the TFLM runtime to manage all intermediate tensors required during inference, including activations, input / output buffers, and temporary layer-specific data. Unlike dynamic memory allocation, which is often unsupported or discouraged in embedded systems, the tensor arena must be pre-allocated at compile time and aligned (typically to 16 bytes) to meet the performance requirements of CMSIS-NN kernels.
    
    Determining the exact memory requirements of the tensor arena in advance is not possible, as it depends on the model architecture and runtime behavior. A practical approach is to consider the available RAM on the microcontroller, subtract the expected system and runtime overhead (will be explained below), and dedicate the remaining memory to the tensor arena. When the device is solely dedicated to model inference, maximizing the tensor arena size enables deployment of more complex models while avoiding allocation failures at runtime.

    In addition, estimating the required tensor arena size—which is always less than the total available RAM of the microcontroller—helps us determine the memory threshold when designing a model for deployment.

  \item \textbf{Operation Resolver:} 
    {\sloppy
    The \texttt{OpResolver} serves as a mapping mechanism that links high-level operations defined in the model—such as \texttt{CONV\_2D}, \texttt{RELU}, or \texttt{SOFTMAX}—to their corresponding low-level implementations. These implementations are typically optimized C or assembly routines (e.g., CMSIS-NN kernels) that execute efficiently on ARM Cortex-M processors by leveraging hardware-specific instructions and memory access patterns.
    \par}


    This is where low-level implementations come in. They are the concrete functions (often hand-optimized in C or assembly) that perform the mathematical computations of each operation. For instance, the high-level \texttt{CONV\_2D} operation gets mapped to a specific optimized kernel like \texttt{arm\_depthwise\_conv\_opt} from the CMSIS-NN library. These kernels are tailored to run efficiently on ARM Cortex-M processors by leveraging hardware-specific instructions and memory access patterns.

    All operation implementations are stored in the Flash memory of the microcontroller. Therefore, it is essential to include only the operations that are strictly necessary for inference, in order to avoid additional Flash memory overhead.

    In our deployment, rather than using tflite::AllOpsResolver—which registers support for all available operators and unnecessarily increases Flash usage—we opted for MicroMutableOpResolver. This allows us to explicitly register only the operations required by our model. This modular approach provides flexibility and enables efficient utilization of memory, which is especially important on resource-constrained microcontrollers

    \clearpage
    
    \item \textbf{Model Data (.h File):} 
    The model data represents a fully quantized neural network in the TensorFlow Lite format. It encapsulates all the essential components of the model, including its architecture (layers and operations), weights, biases, and quantization parameters such as scale factors and zero points. These values are organized in a flat binary format and embedded in the sketch as a constant C array stored in Flash memory.
    
    During execution, this data is interpreted by the TensorFlow Lite runtime to reconstruct the model's internal structure. Each layer—such as convolution, activation, pooling, or softmax—is defined by its configuration and parameters within this binary blob. The interpreter reads and parses this data to initialize tensors, allocate memory, and schedule operations for inference. Thus, the model data acts as the blueprint and numerical foundation of the entire inference process.

    \item \textbf{Interpreter:} 
    The \texttt{tflite::MicroInterpreter} is the central inference engine responsible for executing the model on the microcontroller. It manages the parsing of the model structure, handles memory for all tensors, and coordinates the execution flow of operations layer by layer. It is initialized with four key inputs: the model, an operation resolver, a pre-allocated tensor arena, and the arena size.
    
    Before inference can begin, the interpreter must call \texttt{AllocateTensors()}, which analyzes the model architecture and allocates memory for all input, output, and intermediate tensors within the arena. Once tensors are prepared and inputs are loaded, inference is triggered by calling \texttt{Invoke()}, which executes each layer in sequence and writes the final predictions to the output tensor.
    

    \item \textbf{Input Data:} 
    Instead of reading from sensors, the sketch uses a predefined test image from \texttt{image\_data.h}. The pixel values are assumed to be quantized and directly compatible with the model's input requirements. These are copied into the input tensor buffer obtained from \texttt{interpreter.input(0)}.

    \item \textbf{Inference Execution:} 
    After tensor allocation, the interpreter runs inference using \texttt{interpreter.Invoke()}. The output tensor (obtained via \texttt{interpreter.output(0)}) contains class probabilities or scores. In this sketch, the result is printed to the serial monitor using \texttt{Serial.println()}.
\end{itemize}

This C++ logic bridges the trained model and the embedded runtime, enabling low-power, real-time inference directly on the microcontroller without cloud connectivity or additional hardware support. The modularity of the setup allows easy substitution of input sources or model variants, making it ideal for rapid prototyping and benchmarking.

\section{Evaluation on Device}

Once deployed, the TFLite model is evaluated directly on the Arduino to assess:

\begin{itemize}

\item \textbf{Inference Latency:} Time required for a single prediction.

\item \textbf{RAM \& Flash Usage:} Ensuring the model fits within RAM and Flash constrains.

\item \textbf{Prediction:} We also verify that the model correctly predicts the given input image, even though the accuracy of the quantized TFLite model has already been evaluated in the development environment.

\item \TODO{Current:}


\end{itemize}

Testing was performed using real inputs and logs captured via serial communication to measure performance.

