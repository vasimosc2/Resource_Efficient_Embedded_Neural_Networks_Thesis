\chapter{Literature Review}

\section{Embedded Systems}

Embedded systems are specialized computing systems designed to perform dedicated functions within a larger device or system, often subject to real-time operational constraints. Unlike general-purpose computers, embedded systems are typically built around microcontroller units (MCUs)---single integrated chips that include a processor, memory, and interfaces---allowing them to be highly compact and task-specific.

These systems are characterized by limited resources in terms of processing power, memory, and storage. Moreover, they must often meet strict real-time requirements, meaning they must respond to events or sensor inputs within fixed deadlines to ensure correct overall system behavior \cite{techtarget_embedded_system}.

Because many embedded devices run on battery or have tight power budgets, energy efficiency is a paramount concern. They are designed to operate with minimal power consumption---often on the order of milliwatts or less---while still performing their intended functions.

In summary, an embedded system (especially one based on a microcontroller) tends to prioritize reliability and efficiency for a specific task, trading off raw computing performance for low power operation, small size, and simplicity. Embedded systems are ubiquitous in modern electronics, ranging from household appliances and wearable gadgets to automotive controllers and industrial sensors.

A typical microcontroller-based embedded device might run at only a few tens of megahertz and have memory on the order of kilobytes to a few hundred kilobytes. For example, the Arduino Nano 33 BLE is a representative edge microcontroller platform that features a 32-bit Arm Cortex-M4 CPU running at 64~MHz, with 1~MB of flash storage and only 256~KB of RAM.

These modest specifications underscore the severe resource constraints under which embedded devices operate. Real-time control loops and signal processing tasks are commonly handled on such hardware, but running computationally intensive algorithms (like deep neural networks) is challenging without special optimization.

In many cases, embedded software must be carefully implemented in C/C++ or even assembly, and may run bare-metal or with a lightweight real-time operating system (RTOS), to meet the timing and memory limitations \cite{techtarget_embedded_system}. Furthermore, because the device may be deployed in an unattended environment on a small power source, strategies for conserving energy---such as duty cycling, low-power sleep modes, and efficient use of peripherals---are crucial.

\clearpage

\subsection{Black Box and DARTS}
\TODO{Add it on the Introdcution}
Neural Architecture Search (NAS) is a technique to automatically design neural network architectures. In the context of Convolutional Neural Networks (CNNs), two main NAS paradigms are widely discussed: \textbf{black-box NAS} and \textbf{differentiable NAS}, notably \textit{DARTS} (Differentiable ARchiTecture Search).

Black-box NAS treats the architecture search problem as a discrete, black-box optimization task. This means the internal structure of the architecture is not differentiable and is instead evaluated using methods like reinforcement learning (RL), evolutionary algorithms (EA), or Bayesian optimization~\cite{qiu2023shortest}. Each candidate CNN is trained (fully or partially) and evaluated using a fitness function (e.g., accuracy, latency). Based on these evaluations, the search algorithm proposes new candidates.





\textbf{Advantages:} 
\begin{itemize}
    \item Unrestricted search space; any performance metric or constraint can be handled, making suitable for real AutoML \cite{burrello2023enhancing}.
    \item Offers high flexibility in defining the search space and objective. Specially good when trying to create a model which should fit inside certain hardware constrains.
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
    \item Very expensive in terms of computation; early works required thousands of GPU days~\cite{liu2018darts}.
    \item the final selected architecture usually needs to be trained from scratch to reach peak performance. This step significantly adds to the overall time and compute required, especially when done for each promising candidate. 
\end{itemize}

\begin{comment}
DARTS reformulates NAS as a differentiable optimization problem~\cite{liu2018darts}. Instead of searching in a discrete space, it relaxes architecture choices into continuous parameters that can be optimized via gradient descent.
\end{comment}

\textbf{Definition:} \textit{DARTS} (Differentiable ARchiTecture Search) and related ``one-shot'' methods embed the discrete NAS search into a continuous optimization problem ~\cite{liu2018darts}, introducing DARTS as a way to make NAS tractable for CNN design by relaxing the discrete choices into continuous weights \cite{elsken2019neural}.

Instead of sampling architectures one at a time, DARTS trains a single over-parameterized \textit{supernet} \cite{SuperNet} that contains all candidate operations (e.g., convolutions, pooling) at each layer or edge. Each edge in the CNN cell is modeled as a weighted sum of possible operations:

\begin{equation}
y = \sum_{i=1}^{m} \alpha_i \, o_i(x), \quad \alpha_i \geq 0, \quad \sum_{i} \alpha_i = 1
\end{equation}

where $o_i$ are candidate conv/pool/skip operations and $\alpha_i$ are continuous architecture parameters. These $\alpha_i$ weights serve as proxies for the architecture choice.

\TODO{See that again}

The search is then a \textit{bi-level optimization}: the normal network weights $w$ (for convolution filters) and the architecture weights $\alpha$ are both learned, typically by alternating gradient descent on training and validation data \cite{elsken2019neural}. Once training converges, DARTS discretizes the solution by selecting, for each layer, the operation with the highest weight $\alpha$ – yielding a concrete CNN architecture (often a small ``cell'' that can be stacked).


\textbf{Advantages:}
\begin{itemize}
    \item Significantly more efficient than black-box methods (orders of magnitude less compute)~\cite{liu2018darts}.
    \item Enables fast, gradient-based optimization for cell-based CNN design.
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
    \item \textit{Limited to differentiable and pre-defined search spaces:} DARTS requires that all architecture choices be expressible as continuous and differentiable parameters. This means the engineer must define a fixed search space in advance, such as a set of candidate filter sizes (e.g., 3×3, 5×5) or layer types, which DARTS will then optimize over. As a result, DARTS does not design networks from scratch but rather assists in selecting the best options within a constrained, cell-based framework. This limits its ability to explore broader architectural innovations and makes the process only partially automated~\cite{elsken2019neural}.

    
    \item \textit{Optimization instability and performance collapse:} The bi-level optimization in DARTS is known to be sensitive to the training dynamics. It often leads to degenerate solutions, such as over-prioritizing parameter-free operations like skip connections or pooling layers, resulting in under-parameterized networks~\cite{zela2020understanding}.
    
    \item \textit{Incorporating hard constraints is difficult:} DARTS does not natively support hard architectural constraints, such as strict latency, memory, or energy budgets. These constraints must be approximated using soft regularization terms or multi-objective weighting, which may not guarantee feasibility in real-world deployment scenarios~\cite{king2025micronas}.
    
    \item \textit{Memory and compute bottlenecks:} Since the supernet includes all candidate operations simultaneously, memory usage can become a bottleneck, especially for high-resolution input or larger search spaces. This limits the scalability of DARTS to more complex tasks or deeper networks.
\end{itemize}

\subsection{Comparison Summary}
Black-box NAS methods align with the AutoML paradigm by treating the search process as a black-box optimization task. These methods do not assume any structural knowledge of the network and rely solely on performance metrics to guide the search, thus enabling fully automated exploration of large, complex search spaces. In contrast, differentiable NAS approaches like DARTS introduce a more guided optimization framework by relaxing the architecture search space into continuous parameters. This allows the use of gradients to efficiently steer the search, leading to faster convergence but at the cost of reduced flexibility.







\clearpage





\section{Convolutional Neural Networks (CNNs)}

\TODO{Maybe add a bit extra in this}
Modern artificial neural networks are powerful machine learning models inspired by the human brain, capable of learning complex patterns from data. In particular, Convolutional Neural Networks (CNNs) have become the de facto standard for image recognition and classification tasks.

A CNN is a type of deep neural network designed to process data with a grid-like topology, such as 2D images, automatically learning a hierarchy of feature representations---from low-level edges and textures to high-level semantic objects. The design of CNNs is motivated by the way the animal visual cortex processes visual information, enabling these networks to extract spatial hierarchies of features in an adaptive manner \cite{yamashita2018convolutional}.


\TODO{NAS TO BE ADDED}

\subsection{CNN Architecture}

A typical Convolutional Neural Network (CNN) architecture can be divided into two primary components: \textbf{feature extraction} and \textbf{classification}.

\textbf{Feature Extraction:}
\newline
The feature extraction part consists of a sequence of convolutional layers, each typically followed by a non-linear activation function (such as ReLU6). These convolutional layers apply a series of learnable filters (kernels) to the input data. These filters are generally small in spatial dimensions—commonly 3×3 or 5×5 pixels—and are convolved across the height and width of the input image.

One of the key advantages of this approach is the weight-sharing mechanism, where the same set of filter weights is applied across different spatial locations of the input. This dramatically reduces the number of parameters compared to fully connected layers, while also enabling the model to detect features such as edges and textures regardless of their position in the image (a property known as translation equivariance).

For example, a fully connected layer processing a 100 × 100 image would require 10,000 weights per neuron. In contrast, a convolutional layer using a 5 × 5 kernel requires only 25 weights per filter, highlighting the parameter efficiency of convolutional operations.

Pooling layers, such as max pooling, follow certain convolutional layers to downsamplethe feature maps. This reduces their spatial resolution and introduces translation invariance, meaning the network becomes more robust to small shifts in the input. Pooling also helps to reduce the computational load and mitigate overfitting.

\textbf{Classification:}
\newline
After several convolutional and pooling stages, the network transitions to the classification component. At this stage, the extracted feature maps are either flattened or aggregated using techniques such as Global Average Pooling, and passed to one or more fully connected (dense) layers.

These final layers are responsible for interpreting the high-level features and performing the actual classification task. The network typically concludes with a dense output layer or a global average pooling layer that produces a vector of class scores or probabilities, often using a softmax activation in the case of multi-class classification.


\subsection{CNNs for Image Classification}

CNNs provide several advantages for image classification tasks such as CIFAR-100. They eliminate the need for manual feature engineering by learning complex features directly from data. Their hierarchical feature learning and parameter sharing allow for effective generalization across the image domain.

These properties have enabled CNNs to achieve state-of-the-art accuracy on standard benchmarks such as CIFAR-10/100 and ImageNet. Prominent CNN architectures---including VGG, ResNet, and MobileNet---have demonstrated that increasing network depth and complexity can improve accuracy. However, this often comes at the cost of higher computational and memory demands, which is problematic for deployment on embedded systems.

This tension between accuracy and resource constraints motivates efforts to optimize CNNs for embedded environments, seeking models that maintain performance while minimizing resource usage.

\subsection{Neural Architecture Search (NAS) and CNN Optimization}

The architecture of a CNN---including layer types, filter sizes, number of channels, and depth---directly affects both accuracy and computational cost. Designing optimal CNNs under such constraints is a challenging task. Neural Architecture Search (NAS) offers a solution by automating the exploration of possible architectures to find high-performing models under specific requirements \cite{pau2023quantitative}.

NAS is a specialized AutoML technique that identifies the best neural network configuration for a given task and set of constraints. This is especially relevant for embedded applications, where constraints might include limited parameter count, memory size, inference time, or power consumption.

NAS can incorporate such constraints into its search objective, enabling the discovery of CNN architectures that are suitable for deployment on resource-constrained platforms such as microcontrollers. Recent research, including projects like MCUNet, has shown that NAS can generate compact and efficient CNNs that retain high classification accuracy while fitting within tight hardware budgets. \cite{pau2023quantitative}.

These automatically discovered architectures have demonstrated success on vision tasks like CIFAR-100, operating effectively on low-power microcontroller-class devices.