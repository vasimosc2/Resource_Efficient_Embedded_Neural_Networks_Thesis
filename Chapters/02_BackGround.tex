\chapter{Literature Review}




\section{Embedded Systems}

Embedded systems are specialized computing systems designed to perform dedicated functions within a larger device or system, often subject to real-time operational constraints. Unlike general-purpose computers, embedded systems are typically built around microcontroller units (MCUs)---single integrated chips that include a processor, memory, and interfaces---allowing them to be highly compact and task-specific.

These systems are characterized by limited resources in terms of processing power, memory, and storage. Moreover, they must often meet strict real-time requirements, meaning they must respond to events or sensor inputs within fixed deadlines to ensure correct overall system behavior \cite{techtarget_embedded_system}.

Because many embedded devices run on battery or have tight power budgets, energy efficiency is a paramount concern. They are designed to operate with minimal power consumption---often on the order of milliwatts or less---while still performing their intended functions.

In summary, an embedded system (especially one based on a microcontroller) tends to prioritize reliability and efficiency for a specific task, trading off raw computing performance for low power operation, small size, and simplicity. Embedded systems are ubiquitous in modern electronics, ranging from household appliances and wearable gadgets to automotive controllers and industrial sensors.

A typical microcontroller-based embedded device might run at only a few tens of megahertz and have memory on the order of kilobytes to a few hundred kilobytes. For example, the Arduino Nano 33 BLE is a representative edge microcontroller platform that features a 32-bit Arm Cortex-M4 CPU running at 64~MHz, with 1~MB of flash storage and only 256~KB of RAM.

These modest specifications underscore the severe resource constraints under which embedded devices operate. Real-time control loops and signal processing tasks are commonly handled on such hardware, but running computationally intensive algorithms (like deep neural networks) is challenging without special optimization.

In many cases, embedded software must be carefully implemented in C/C++ or even assembly, and may run bare-metal or with a lightweight real-time operating system (RTOS), to meet the timing and memory limitations \cite{techtarget_embedded_system}. Furthermore, because the device may be deployed in an unattended environment on a small power source, strategies for conserving energy---such as duty cycling, low-power sleep modes, and efficient use of peripherals---are crucial.

\clearpage








\section{Introduction to Neural Architecture Search}
\label{chap:nas}
Neural Architecture Search (NAS) refers to the automated design of neural network architectures, aiming to replace laborious manual tuning by human experts \cite{elsken2019neural}. As a subfield of AutoML, NAS can be viewed along three key dimensions: the search space, the search strategy, and the performance estimation method~\cite{elsken2019neural}. In recent years, NAS has demonstrated its ability to discover architectures that outperform manually designed models on benchmarks such as CIFAR-10 and ImageNet.

\subsection{Default NAS: Black-Box Approach}
The default form of NAS is typically \textbf{black-box NAS}, which treats the architecture search process as a discrete and non-differentiable optimization task. In this setting, the internal structure of the architecture is opaque to the optimizer: each candidate model is treated as a "black box" that is trained and evaluated using a predefined fitness function (e.g., accuracy, latency, model size).

Popular optimization strategies for black-box NAS include reinforcement learning (RL), evolutionary algorithms (EA), and Bayesian optimization~\cite{qiu2023shortest}. These methods work by sampling and evaluating entire architectures, and then using performance feedback to guide future exploration.

\textbf{Advantages:}
\begin{itemize}
    \item Flexible and expressive: can handle arbitrary architectures and constraints (e.g., latency, energy, memory).
    \item Supports any performance metric, not limited to differentiable objectives.
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
    \item Extremely computationally expensive; early approaches required thousands of GPU days~\cite{liu2018darts}.
    
\end{itemize}

Because of these computational challenges, several optimization techniques have been proposed to accelerate NAS without sacrificing too much performance. The two most prominent approaches are \textbf{DARTS} and \textbf{Supernet-based NAS}, both of which aim to make NAS more tractable by sharing weights across candidate architectures.

\subsection{Optimization Technique 1: Differentiable NAS (DARTS)}
\textit{DARTS} (Differentiable ARchiTecture Search) transforms the discrete NAS problem into a continuous one by introducing a set of differentiable architecture parameters~\cite{liu2018darts}. Instead of evaluating architectures one at a time, DARTS constructs a single \textit{over-parameterized supernet} that contains all candidate operations at each layer or edge. Each operation is associated with a continuous weight $\alpha_i$ representing its importance:

\begin{equation}
y = \sum_{i=1}^{m} \alpha_i \, o_i(x), \quad \alpha_i \geq 0, \quad \sum_{i} \alpha_i = 1
\end{equation}

This enables gradient-based optimization over both:
\begin{itemize}
    \item The standard network weights $w$ (e.g., convolution filters), and
    \item The architecture weights $\alpha$.
\end{itemize}

The search process is a \textit{bi-level optimization}: weights $w$ are trained on training data, while architecture weights $\alpha$ are optimized on validation data. Once the search converges, the architecture is discretized by selecting, for each layer, the operation with the highest $\alpha_i$.

\textbf{What DARTS Does:}
\begin{itemize}
    \item Builds a supernet with all possible operations active via softmax weights.
    \item Learns both model parameters and architecture jointly via gradient descent.
    \item After training, selects the most likely operation per layer (argmax of $\alpha$).
\end{itemize}

\textbf{Strengths:}
\begin{itemize}
    \item Much faster than black-box NAS (orders of magnitude fewer evaluations).
    \item Enables efficient gradient-based architecture search.
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
    \item Only works for differentiable and pre-defined search spaces.
    \item Cannot easily incorporate hard hardware constraints (e.g., RAM/Flash).
    \item Often suffers from optimization instability and performance collapse~\cite{zela2020understanding}.
    \item Requires retraining the final architecture after discretization.
\end{itemize}

\subsection{Optimization Technique 2: Supernet-Based NAS}
Supernet-based NAS techniques such as Once-for-All~\cite{cai2019once} and BigNAS~\cite{yu2020bignas} train a single, unified \textbf{supernet} that includes all candidate subarchitectures. Unlike DARTS, which mixes all operations during each forward pass, these methods activate only \textit{one subnetwork at a time} during training. Subnetworks share weights where applicable, making training efficient.

\textbf{What Supernet-Based NAS Does:}
\begin{itemize}
    \item Trains one large supernet that includes all subnetworks.
    \item At each training step, samples and trains a single subnetwork (one path).
    \item After training, subarchitectures can be extracted and evaluated without retraining.
\end{itemize}

\textbf{Key Techniques:}
\begin{itemize}
    \item \textit{Weight sharing:} Subnets inherit trained weights from the supernet.
    \item \textit{Progressive shrinking:} Trains large subnets first, then adapts to smaller ones (e.g., lower width or depth).
\end{itemize}

\textbf{Strengths:}
\begin{itemize}
    \item Extremely fast subnet evaluation — no need to retrain.
    \item Naturally supports hardware-aware search (latency, RAM, Flash).
    \item Subnet evaluation is realistic, since only one op is active at a time.
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
    \item Requires careful training to ensure fair weight sharing.
    \item Subnets may suffer from gradient interference; performance may not fully match retrained models.
    \item Fine-tuning may be required for deployment-quality accuracy.
\end{itemize}


\begin{comment}
\subsection{Summary and Thesis Direction}
Black-box NAS offers maximum flexibility and supports arbitrary performance constraints, but suffers from high computational cost. DARTS introduces differentiability to reduce this cost but trades off flexibility and realism. Supernet-based methods strike a middle ground, enabling efficient and hardware-aware search through weight sharing and path sampling.
\end{comment}

\vspace{0.3em}
\noindent In this thesis, we adopt a \textbf{black-box NAS approach} using a \textbf{Genetic Algorithm (GA)} to optimize model architecture. This allows us to:
\begin{itemize}
    \item Directly incorporate hard constraints like RAM and Flash memory.
    \item Evaluate fully instantiated models using custom estimators.
    \item Search in an unconstrained space without relying on predefined, differentiable operations.
\end{itemize}

This approach balances flexibility, deployment realism, and scalability — making it ideal for creating efficient, hardware-aware models in resource-constrained environments.














\section{Convolutional Neural Networks (CNNs)}

In recent years, artificial neural networks (ANNs) have emerged as powerful tools in the field of machine learning, particularly in applications involving complex and high-dimensional data  \cite{lecun2015deep}. Inspired by the structure and function of the human brain, these networks are capable of learning intricate patterns and relationships from raw input data through a process known as representation learning  \cite{bengio2013representation}. Among the various architectures developed, Convolutional Neural Networks (CNNs) have proven to be especially effective in handling tasks related to image recognition, classification, and computer vision \cite{krizhevsky2017imagenet, yamashita2018convolutional}.
CNNs are a class of deep learning models specifically designed to process data with a grid-like topology, such as 2-dimensional (2D) images. Unlike traditional fully connected neural networks, where each neuron is connected to every neuron in the previous layer, CNNs exploit the spatial structure of image data through local connections, shared weights, and translation invariance. \cite{yamashita2018convolutional}.


\subsection{Biological Motivation}
The design of CNNs is inspired by the visual processing mechanisms observed in the animal visual cortex, particularly the primary visual cortex (V1). Neuroscientific studies have shown that individual neurons in the visual cortex respond to specific regions of the visual field and are sensitive to certain features such as edges, orientations, and textures. These neurons are organized hierarchically: lower-level neurons detect simple patterns, while higher-level neurons combine those patterns to recognize complex shapes and objects.

Similarly, CNNs are structured in such a way that earlier layers learn to detect low-level visual features (e.g., edges and corners), while deeper layers capture high-level representations, such as object parts or even entire objects.


\subsection{CNN Architecture}

A typical Convolutional Neural Network (CNN) architecture can be divided into two primary components: \textbf{feature extraction} and \textbf{classification}.

\textbf{Feature Extraction:}
\newline
The feature extraction part consists of a sequence of \textbf{convolutional layers}, each typically followed by a non-linear activation function (such as ReLU6). These convolutional layers apply a series of learnable filters (kernels) to the input data,producing feature maps that highlight the presence of specific patterns or features. Each filter slides (convolves) across the input image, computing the dot product between the filter and local regions of the input. These filters are generally small in spatial dimensions—commonly 3×3 or 5×5 pixels—and are convolved across the height and width of the input image.

After the convolution operation, an \textbf{activation function }(typically ReLU, or Rectified Linear Unit) is applied to introduce non-linearity into the model. This allows the network to learn complex patterns that are not limited to linear transformations.

One of the key advantages of this approach is the weight-sharing mechanism, where the same set of filter weights is applied across different spatial locations of the input. This dramatically reduces the number of parameters compared to fully connected layers, while also enabling the model to detect features such as edges and textures regardless of their position in the image (a property known as translation equivariance).

For example, a fully connected layer processing a 100 × 100 image would require 10,000 weights per neuron. In contrast, a convolutional layer using a 5 × 5 kernel requires only 25 weights per filter, highlighting the parameter efficiency of convolutional operations.

\textbf{Pooling layers} (also known as subsampling or downsampling layers), such as max pooling, follow certain convolutional layers to downsamplethe feature maps. This reduces their spatial resolution and introduces translation invariance, meaning the network becomes more robust to small shifts in the input. Pooling also helps to reduce the computational load and mitigate overfitting.

\textbf{Classification:}
\newline
After several convolutional and pooling stages, the network transitions to the classification component. At this stage, the extracted feature maps are either flattened or aggregated using techniques such as Global Average Pooling, and passed to one or more \textbf{fully connected (dense) layers}.

These final layers are responsible for interpreting the high-level features and performing the actual classification task. The network typically concludes with a dense \textbf{output layer }or a global average pooling layer that produces a vector of class scores or probabilities, often using a softmax activation in the case of multi-class classification.


\subsection{CNNs for Image Classification}

CNNs provide several advantages for image classification tasks such as CIFAR-100. They eliminate the need for manual feature engineering by learning complex features directly from data. Their hierarchical feature learning and parameter sharing allow for effective generalization across the image domain.

These properties have enabled CNNs to achieve state-of-the-art accuracy on standard benchmarks such as CIFAR-10/100 and ImageNet. Prominent CNN architectures---including VGG, ResNet, and MobileNet---have demonstrated that increasing network depth and complexity can improve accuracy. However, this often comes at the cost of higher computational and memory demands, which is problematic for deployment on embedded systems.

This tension between accuracy and resource constraints motivates efforts to optimize CNNs for embedded environments, seeking models that maintain performance while minimizing resource usage.

\subsection{Key Advantages of CNNs}
CNNs offer several advantages over traditional machine learning models in the context of image processing:
\begin{itemize}
    \item \textbf{Automatic Feature Extraction}: CNNs eliminate the need for manual feature engineering by automatically learning relevant features from the data during training.
    \item \textbf{Parameter Efficiency}: Through the use of shared weights and local connectivity, CNNs significantly reduce the number of parameters, improving training efficiency and reducing the risk of overfitting.
    \item \textbf{Translation Invariance}: Because of the convolutional architecture and pooling layers, CNNs are naturally invariant to small translations, rotations, and distortions in the input data.
    \item \textbf{Hierarchical Feature Learning}: CNNs are capable of learning features at multiple levels of abstraction, from simple edges to complex objects.

\end{itemize}

\subsection{Applications of CNNs}
Convolutional Neural Networks have become a fundamental building block in the field of computer vision due to their exceptional ability to extract spatial hierarchies of features from image data. Their architecture is particularly well-suited for interpreting visual information, making them indispensable in numerous real-world applications across industries. Below are some of the most prominent domains where CNNs are applied:

\begin{enumerate}
    \item \textbf{Image Classification}
    
    CNNs are widely used in image classification tasks, where the goal is to assign a single label to an entire image based on its content.
    \begin{itemize}
    
        \item \textbf{Example}: Classifying images into categories such as cats, dogs, airplanes, or flowers.
        \item CNNs automatically learn to recognize distinguishing features—such as shapes, colors, and textures—that differentiate one class from another.
        \item Popular datasets: ImageNet, CIFAR-10, MNIST.
        
    \end{itemize}
    
    \item \textbf{Object Detection}
    
    Unlike image classification, object detection not only identifies the presence of objects but also locates them within the image using bounding boxes.
    
    \begin{itemize}
        \item \textbf{Example}: Detecting pedestrians, vehicles, and traffic signs in real-time video feeds.
        \item CNN-based models like YOLO (You Only Look Once), Faster R-CNN, and SSD (Single Shot Multibox Detector) are state-of-the-art in this field.
        \item pplications range from surveillance systems to retail analytics and robotics.




    \end{itemize}
    
    \item \textbf{Facial Recognition}
    
    CNNs play a central role in systems that recognize or verify human identities based on facial features.
    \begin{itemize}
        \item \textbf{Example}: Unlocking smartphones using face ID or verifying users at border security checkpoints.
        \item CNNs extract unique facial embeddings that can be matched against a database using techniques such as FaceNet or DeepFace.
        \item Applications span biometrics, surveillance, social media, and authentication.




    \end{itemize}
   
    \item \textbf{Medical Image Analysis}


    In healthcare, CNNs are increasingly used to assist in diagnosing diseases from medical images such as X-rays, MRIs, CT scans, and histopathology slides.
    \begin{itemize}
        \item \textbf{Example}: Detecting tumors, fractures, or abnormalities in radiological scans.
        \item CNNs can outperform human experts in specific diagnostic tasks and provide decision support to radiologists and clinicians.
        \item Common use cases: cancer detection, organ segmentation, disease progression monitoring.
    \end{itemize}
   
    \item \textbf{Autonomous Driving}
    Self-driving vehicles rely heavily on CNNs to interpret their surroundings in real time.
    \begin{itemize}
        \item \textbf{Example}: Identifying lanes, pedestrians, obstacles, and traffic signs to make driving decisions.
        \item CNNs are used in conjunction with sensors like LiDAR and radar to create a complete scene understanding.
        \item Real-time performance and robustness to varying conditions (e.g., weather, lighting) are critical challenges.
    \end{itemize}
\end{enumerate}
\subsection{Neural Architecture Search (NAS) and CNN Optimization}

The architecture of a CNN---including layer types, filter sizes, number of channels, and depth---directly affects both accuracy and computational cost. Designing optimal CNNs under such constraints is a challenging task. Neural Architecture Search (NAS) offers a solution by automating the exploration of possible architectures to find high-performing models under specific requirements \cite{pau2023quantitative}.

NAS is a specialized AutoML technique that identifies the best neural network configuration for a given task and set of constraints. This is especially relevant for embedded applications, where constraints might include limited parameter count, memory size, inference time, or power consumption.

NAS can incorporate such constraints into its search objective, enabling the discovery of CNN architectures that are suitable for deployment on resource-constrained platforms such as microcontrollers. Recent research, including projects like MCUNet, has shown that NAS can generate compact and efficient CNNs that retain high classification accuracy while fitting within tight hardware budgets. \cite{pau2023quantitative}.

These automatically discovered architectures have demonstrated success on vision tasks like CIFAR-100, operating effectively on low-power microcontroller-class devices.

\subsection{TakuNet}

In this thesis, we adopted \textbf{TakuNet}—a lightweight convolutional neural network architecture originally proposed for real-time aerial image classification in emergency response scenarios \cite{TakuNet}. TakuNet was explicitly designed to operate efficiently on embedded systems such as the NVIDIA Jetson Orin Nano and Raspberry Pi, while achieving high accuracy on aerial datasets like AIDER and AIDERv2.

The original TakuNet architecture employs depth-wise separable convolutions and an early downsampling stem to reduce spatial dimensions in the initial layers, significantly lowering computational complexity. It also integrates dense connections for improved gradient flow and convergence, grouped point-wise convolutions to reduce parameter count, and Global Response Normalization (GRN) to enhance feature diversity and contrast. Furthermore, the model is trained using half-precision floating point (FP16), which improves compatibility with hardware accelerators like NVIDIA TensorRT.

In this work, we adopted TakuNet as a \textbf{baseline architecture and design template} due to its promising trade-off between accuracy, parameter count, and computational efficiency. Although the original implementation demonstrated competitive performance when compared to other state-of-the-art models (see Table~\ref{tab:model_comparison}), its parameter size and memory footprint remained too large for direct deployment on typical microcontroller-class devices. 

To address these limitations while retaining the core strengths of the architecture, we employed a \textbf{Neural Architecture Search (NAS)} algorithm to systematically refine both the macro- and micro-architecture of TakuNet. The goal was to tailor the network to our specific datasets and hardware constraints, achieving efficient inference without sacrificing classification performance.

Further architectural details, along with additional enhancements aimed at improving validation accuracy within a limited number of training epochs, are provided in Chapter~\ref{chap:Architecture}.


\begin{table}[h!]
\centering
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Parameters} & \textbf{Model Size (MB)} & \textbf{F1-score} & \textbf{FLOPS} \\
\hline
VGG16 \cite{simonyan2014very} & 14,840,133 & 59.36 & 0.601 & 17.62G \\
ResNet50 \cite{he2016deep} & 23,518,277 & 94.07 & 0.917 & 4.83G \\
SqueezeNet \cite{iandola2016squeezenet} & 737,989 & 2.95 & 0.890 & 845.38M \\
EfficientNet B0 \cite{tan2019efficientnet} & 4,013,953 & 15.66 & 0.905 & 479.68M \\
MobileNetV2 \cite{sandler2018mobilenetv2} & 2,203,277 & 8.92 & 0.930 & 71.82M \\
MobileNetV3 \cite{howard2019searching} & 4,208,437 & 16.08 & 0.915 & 206.22M \\
ShuffleNet \cite{zhang2018shufflenet} & 1,258,729 & 5.03 & 0.908 & 176.82M \\
EmergencyNet \cite{emergencynet} & 90,963 & 0.360 & 0.896 & 77.34M \\
TinyEmergencyNet \cite{tinyemergencynet} & 39,334 & 0.160 & 0.895 & 36.30M \\
\textbf{TakuNet$_\text{fp16}$ (ours)} & \textbf{37,685} & \textbf{0.15} & \textbf{0.943} & \textbf{35.93M} \\
\hline
\end{tabular}
\caption{Comparison of model parameters, size, F1-score, and FLOPs on AIDER.}
\label{tab:model_comparison}
\end{table}


