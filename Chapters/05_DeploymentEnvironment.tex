\chapter{Deployment Environment}
\label{chap:DeploymentEnvironment}
\section{Description}

In this thesis, our objective is to deploy the proposed model on a highly constrained edge device\textemdash the \textbf{Arduino Nano 33 BLE Sense}. This microcontroller board features an Arm Cortex-M4 CPU with limited RAM and Flash memory, making it a challenging yet meaningful platform for evaluating the efficiency of neural network architectures optimized via Neural Architecture Search (NAS). The primary goal is to demonstrate that a well-structured lightweight model can achieve high accuracy while remaining deployable on low-power hardware.



\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{Pictures/Arduino image.png}
    \caption{Example of an Arduino Nano family microcontroller board. Modern TinyML deployments target devices like the Arduino Nano 33 BLE Sense, which feature low-power CPUs but only on the order of a few hundred kilobytes of SRAM and a few megabytes of flash memory. }
    \label{fig:arduino_nano}
\end{figure}






\section{Arduino Nano 33 BLE Sense Specifications}

The Arduino Nano 33 BLE Sense is a compact, low-power board designed for embedded AI applications. The following are the key technical specifications:

\begin{itemize}
\item \textbf{Processor:} 64 MHz Arm Cortex-M4 CPU with FPU
\item \textbf{RAM:} 256 KB
\item \textbf{Flash Memory:} 1 MB
\item \textbf{Connectivity:} Bluetooth 5.0 Low Energy
\item \textbf{Sensors:} 9-axis IMU, microphone, temperature, humidity, pressure, color, gesture
\item \textbf{Power Consumption:} Extremely low, suitable for battery-powered deployments
\end{itemize}

Due to these hardware constraints, deploying deep learning models on this device requires careful optimization of memory usage and computational complexity. 

The model itself—meaning the weights and the compiled operations—is stored in the non-volatile \textbf{Flash memory}. Therefore, the total model size must not exceed the available 1 MB of Flash. In contrast, \textbf{RAM} is volatile and used at runtime to store intermediate tensors, activations, input/output buffers, and temporary computation data. During inference, a structure known as the \texttt{tensor arena} is allocated in RAM to support these operations. This arena must be statically pre-allocated and sized conservatively to avoid memory allocation failures, while still allowing the model to run effectively.

Understanding the difference between these two memory types is crucial:
\begin{itemize}
\item \textbf{Flash memory} retains data after power-off and is used to store the model and program code.
\item \textbf{RAM} is cleared on reboot and is used dynamically during inference for data processing.
\end{itemize}

Given the limited 256 KB of RAM, a large portion must be reserved for the tensor arena. This requires balancing model complexity with available runtime memory. Optimizing both Flash and RAM usage is essential to ensure the model fits within the constraints of the device and executes reliably.


\section{Deployment Process}
\label{sec:DeploymentProcess}
To deploy the quantized TensorFlow Lite (TFLite) model on the Arduino Nano 33 BLE Sense, a C++ sketch is written in the \texttt{Arduino IDE}. This sketch integrates TensorFlow Lite for Microcontrollers (TFLM) and includes logic to load the model, allocate memory, run inference, and output results. 

The critical components of this deployment are:

\begin{itemize}
    \item \textbf{Tensor Arena:}  
    A statically allocated memory buffer used by the TensorFlow Lite for Microcontrollers (TFLM) runtime to manage all intermediate tensors during inference. This includes activations, input/output buffers, and temporary layer-specific data. Unlike dynamic memory allocation, which is often unsupported or discouraged in embedded environments, the tensor arena must be pre-allocated at compile time and aligned (typically to 16 bytes) to meet the performance and alignment requirements of CMSIS-NN kernels.
    
    The exact memory requirements of the tensor arena cannot be determined analytically in advance, as they depend on both the model architecture and runtime tensor operations. A practical approach is to first determine the total available RAM on the microcontroller, subtract any system and runtime overhead (explained later), and allocate the remaining memory to the tensor arena. When the device is dedicated solely to running the model, maximizing the tensor arena size allows for the deployment of more complex architectures while avoiding runtime allocation failures.
    
    In our experiments, the tensor arena was allocated using the **maximum available RAM** of the Arduino Nano BLE 33, after accounting for system overhead. This ensured that the largest possible memory space was reserved for model inference, enabling the evaluation of more expressive candidate architectures during the NAS process.

    If a model requires more memory than what the tensor arena can provide—due to its size or internal tensor complexity—the deployment will fail at runtime with an allocation error. Specifically, the TFLM runtime will be unable to initialize necessary buffers, resulting in execution failure or a hard fault. This makes it critical to accurately estimate memory requirements during the NAS process and discard models that exceed the deployable memory threshold before attempting deployment.
    

  \item \textbf{Operation Resolver:} 
    {\sloppy
    The \texttt{OpResolver} serves as a mapping mechanism that links high-level operations defined in the model—such as \texttt{CONV\_2D}, \texttt{RELU}, or \texttt{SOFTMAX}—to their corresponding low-level implementations. These implementations are typically optimized C or assembly routines (e.g., CMSIS-NN kernels) that execute efficiently on ARM Cortex-M processors by leveraging hardware-specific instructions and memory access patterns.
    \par}


    This is where low-level implementations come in. They are the concrete functions (often hand-optimized in C or assembly) that perform the mathematical computations of each operation. For instance, the high-level \texttt{CONV\_2D} operation gets mapped to a specific optimized kernel like \texttt{arm\_depthwise\_conv\_opt} from the CMSIS-NN library. These kernels are tailored to run efficiently on ARM Cortex-M processors by leveraging hardware-specific instructions and memory access patterns.

    All operation implementations are stored in the Flash memory of the microcontroller. Therefore, it is essential to include only the operations that are strictly necessary for inference, in order to avoid additional Flash memory overhead.

    In our deployment, rather than using tflite::AllOpsResolver—which registers support for all available operators and unnecessarily increases Flash usage—we opted for MicroMutableOpResolver. This allows us to explicitly register only the operations required by our model. This modular approach provides flexibility and enables efficient utilization of memory, which is especially important on resource-constrained microcontrollers
    
    \item \textbf{Model Data (.h File):} 
    The model data represents a fully quantized neural network in the TensorFlow Lite format. It encapsulates all the essential components of the model, including its architecture (layers and operations), weights, biases, and quantization parameters such as scale factors and zero points. These values are organized in a flat binary format and embedded in the sketch as a constant C array stored in Flash memory.
    
    During execution, this data is interpreted by the TensorFlow Lite runtime to reconstruct the model's internal structure. Each layer—such as convolution, activation, pooling, or softmax—is defined by its configuration and parameters within this binary blob. The interpreter reads and parses this data to initialize tensors, allocate memory, and schedule operations for inference. Thus, the model data acts as the blueprint and numerical foundation of the entire inference process.

    \item \textbf{Interpreter:} 
    The \texttt{tflite::MicroInterpreter} is the central inference engine responsible for executing the model on the microcontroller. It manages the parsing of the model structure, handles memory for all tensors, and coordinates the execution flow of operations layer by layer. It is initialized with four key inputs: the model, an operation resolver, a pre-allocated tensor arena, and the arena size.
    
    Before inference can begin, the interpreter must call \texttt{AllocateTensors()}, which analyzes the model architecture and allocates memory for all input, output, and intermediate tensors within the arena. Once tensors are prepared and inputs are loaded, inference is triggered by calling \texttt{Invoke()}, which executes each layer in sequence and writes the final predictions to the output tensor.
    

    \item \textbf{Input Data:} 
    Instead of acquiring real-time data from sensors, the deployment sketch uses a predefined test image included in the source file \texttt{image\_data.h}. This image is typically represented as a \texttt{const uint8\_t[]} array, and it is embedded into the binary at compile time. As a result, it is stored in the device’s non-volatile Flash memory.
    
    At runtime, the pixel values from this array—already quantized and formatted to match the model’s input requirements (e.g., shape, datatype, and scale)—are copied into the input tensor buffer obtained from \texttt{interpreter.input(0)}. This buffer is part of the tensor arena, which resides in RAM. The copying is performed explicitly in a loop or via a call to \texttt{memcpy}, for example:
    
    \begin{lstlisting}[language=C, caption={Copying input image data from Flash to RAM input tensor}, label=lst:copy_input_tensor]
    for (int i = 0; i < 3072; i++) {
        input->data.uint8[i] = IMAGE_NAME[i];
    }
    \end{lstlisting}

    
    This means that although the test image consumes no RAM while stored in Flash, it does occupy RAM during inference. Specifically, the input tensor consumes a portion of the tensor arena, which also holds all intermediate activations, output tensors, and temporary buffers required by the model. For example, a \(32 \times 32 \times 3\) image with 8-bit quantization requires exactly 3,072 bytes of RAM during inference.
    
    This memory footprint must be considered when allocating the tensor arena. If the input tensor, combined with the rest of the model’s memory requirements, exceeds the available RAM, the inference will fail due to insufficient space. Therefore, even for a predefined input, its RAM usage at runtime directly affects deployment feasibility on memory-constrained microcontrollers.


    \item \textbf{Inference Execution:} 
    Once the input data has been copied into the input tensor and memory has been allocated via \texttt{AllocateTensors()}, the interpreter executes the model by calling \texttt{interpreter.Invoke()}. This function runs the full inference pipeline, layer by layer, using the model loaded in memory and the pre-allocated tensor arena.
    
    After inference, the result is stored in the output tensor, which can be accessed using \texttt{interpreter.output(0)}. This tensor typically contains the model’s predicted output, such as class probabilities or regression values, depending on the task. In this sketch, the output is assumed to represent a probability distribution over classes. The values are read from the output tensor and printed to the serial monitor using \texttt{Serial.print()} and \texttt{Serial.println()} for inspection. This setup allows the user to verify model predictions on the predefined test input without requiring any external output hardware.
    
    \clearpage
    
    \begin{lstlisting}[language=C, caption={Running inference and printing the result to the serial monitor}, label=lst:run_inference_print_output]
    if (interpreter.Invoke() != kTfLiteOk) {
        Serial.println("Inference failed!");
        return;
    }
    
    // Access output tensor
    TfLiteTensor* output = interpreter.output(0);
    for (int i = 0; i < output->dims->data[1]; i++) {
        Serial.print("Class ");
        Serial.print(i);
        Serial.print(": ");
        Serial.println(output->data.uint8[i]);
    }
    \end{lstlisting}

    
\end{itemize}

This C++ logic bridges the trained model and the embedded runtime, enabling low-power, real-time inference directly on the microcontroller without cloud connectivity or additional hardware support. The modularity of the setup allows easy substitution of input sources or model variants, making it ideal for rapid prototyping and benchmarking.

\section{Evaluation on Device}

Once deployed, the TFLite model is evaluated directly on the Arduino to assess:

\begin{itemize}

\item \textbf{Inference Latency:} Time required for a single prediction.

\item \textbf{RAM \& Flash Usage:} Ensuring the model fits within RAM and Flash constrains.

\item \textbf{Prediction:} We also verify that the model correctly predicts the given input image, even though the accuracy of the quantized TFLite model has already been evaluated in the development environment.

\item \textbf{Power:} The microcontroller is powered with a stable 3.3\,V supply provided by the PPK2 in source meter mode. During a single inference, the PPK2 records the current draw over time, allowing precise measurement of the power consumption and computation of the total energy used for one inference cycle.



\end{itemize}

Testing was performed using real inputs and logs captured via serial communication to measure performance.

