\section*{Abstract}



\addcontentsline{toc}{section}{Abstract}



Deploying deep learning models on resource-constrained embedded platforms, such as the Arduino Nano BLE 33, presents significant challengesâ€”particularly in balancing model accuracy, memory footprint, and computational efficiency. This thesis addresses these challenges by accelerating the Neural Architecture Search (NAS) process to rapidly identify lightweight, high-performing architectures tailored for ultra-low-power devices.

The proposed approach introduces TakuNet, a custom convolutional neural network (CNN) architecture optimized for efficient inference. To explore the architectural search space effectively, an Evolutionary Algorithm is employed, leveraging genetic operations such as mutation and crossover. To further enhance search efficiency, a RankNet-based surrogate model is integrated to predict the relative performance of candidate architectures, significantly reducing the need for exhaustive model training.

The framework also includes intelligent memory estimation and a complete model quantization workflow to ensure continuous compliance with the Arduino Nano BLE 33's hardware constraints. Efficient training strategies and dynamic resource management are implemented to maximize the number of architectures evaluated within strict time and resource budgets.

Final models are fully quantized and converted into C array formats for seamless deployment on the target hardware. Experimental results demonstrate that the proposed accelerated NAS methodology successfully produces high-performance models suitable for deployment in highly resource-constrained environments.







