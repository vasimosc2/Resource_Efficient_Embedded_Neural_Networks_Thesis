\section*{Motivation}
\addcontentsline{toc}{section}{Motivation}
Deep learning has demonstrated remarkable success across various domains, primarily due to its ability to automatically learn powerful feature representations from data. A key determinant of a model’s performance lies in the design of its neural architecture. Traditionally, this design process has relied heavily on expert intuition, prior knowledge, and manual experimentation—making it both time-consuming and suboptimal.

Neural Architecture Search (NAS) emerged as a groundbreaking approach to automate the design of neural networks, significantly reducing the need for human intervention. By systematically exploring vast architecture spaces, NAS has the potential to discover highly effective network structures. However, this promise comes with a major drawback: extremely high computational cost. The search process can span days or even weeks, depending on the size of the search space and the evaluation strategy used. As a result, NAS is often impractical in real-world scenarios where resources and time are limited. \cite{Wang_2020_CVPR} 

This thesis is motivated by the need to make NAS more efficient and reliable. Specifically, it aims to explore novel methods and correlations that can serve as proxies for expensive evaluations, reduce redundant computations, and guide the search more intelligently. By incorporating such modifications, the goal is to develop a NAS pipeline that not only maintains or improves performance but also significantly reduces the time and resources required. In doing so, this work contributes towards making NAS a more feasible and scalable solution for real-world deep learning applications.